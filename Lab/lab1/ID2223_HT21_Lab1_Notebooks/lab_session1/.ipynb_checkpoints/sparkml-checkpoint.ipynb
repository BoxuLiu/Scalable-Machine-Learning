{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID2223 HT21 Lab Session 1\n",
    "Working with Spark:\n",
    "- DataFrame\n",
    "- Text Processing Example \n",
    "  - Individual Stages\n",
    "  - Pipeline\n",
    "- Feature Engineering \n",
    "  - Vector Assembler\n",
    "  - Continues Features\n",
    "  - Categorical Features\n",
    "  - Text Features\n",
    "- Linear regression\n",
    "- Binary classification\n",
    "- Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://e38e74d79d1c:4040\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1636700527321)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "people: org.apache.spark.sql.DataFrame = [age: bigint, id: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val spark: SparkSession = SparkSession.builder().master(\"local\").appName(\"test\").getOrCreate()\n",
    "\n",
    "val people = spark.read.format(\"json\").load(\"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 30| 15|   Andy|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 3)|\n",
      "+-------+---------+\n",
      "|Michael|       18|\n",
      "|   Andy|       33|\n",
      "| Justin|       22|\n",
      "|   Andy|       15|\n",
      "|    Jim|       22|\n",
      "|   Andy|       15|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "people.select(col(\"name\"), expr(\"age + 3\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 3)|\n",
      "+-------+---------+\n",
      "|Michael|       18|\n",
      "|   Andy|       33|\n",
      "| Justin|       22|\n",
      "|   Andy|       15|\n",
      "|    Jim|       22|\n",
      "|   Andy|       15|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "people.select($\"name\", $\"age\" + 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 15| 12|Michael|\n",
      "| 19| 20| Justin|\n",
      "| 12| 15|   Andy|\n",
      "| 19| 20|    Jim|\n",
      "| 12| 10|   Andy|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.where(\"age < 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 4\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.select(\"name\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+\n",
      "|age| id|   name|teenager|\n",
      "+---+---+-------+--------+\n",
      "| 15| 12|Michael|    true|\n",
      "| 30| 15|   Andy|   false|\n",
      "| 19| 20| Justin|    true|\n",
      "| 12| 15|   Andy|    true|\n",
      "| 19| 20|    Jim|    true|\n",
      "| 12| 10|   Andy|    true|\n",
      "+---+---+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"teenager\", expr(\"age < 20\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[String] = Array(age, id, username)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.withColumnRenamed(\"name\", \"username\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(age)|\n",
      "+----------+\n",
      "|         6|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(count(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|first(name)|last(age)|\n",
      "+-----------+---------+\n",
      "|    Michael|       12|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(first(\"name\"), last(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(age)|\n",
      "+------------------+\n",
      "|17.833333333333332|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|count(age)|\n",
      "+-------+----------+\n",
      "|    Jim|         1|\n",
      "|Michael|         1|\n",
      "|   Andy|         3|\n",
      "| Justin|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(\"name\").agg(count(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---+-----+\n",
      "|num|name| id| id|group|\n",
      "+---+----+---+---+-----+\n",
      "|  0|   a|  0|  0|    x|\n",
      "|  1|   b|  1|  1|    y|\n",
      "|  2|   c|  1|  1|    y|\n",
      "+---+----+---+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1: org.apache.spark.sql.DataFrame = [num: int, name: string ... 1 more field]\n",
       "t2: org.apache.spark.sql.DataFrame = [id: int, group: string]\n",
       "joinExpression: org.apache.spark.sql.Column = (id = id)\n",
       "joinType: String = inner\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1 = spark.createDataFrame(Seq((0, \"a\", 0), (1, \"b\", 1), (2, \"c\", 1))).toDF(\"num\", \"name\", \"id\")\n",
    "val t2 = spark.createDataFrame(Seq((0, \"x\"), (1, \"y\"), (2, \"z\"))).toDF(\"id\", \"group\")\n",
    "\n",
    "val joinExpression = t1.col(\"id\") === t2.col(\"id\")\n",
    "var joinType = \"inner\"\n",
    "\n",
    "t1.join(t2, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| text|upper|\n",
      "+---+-----+-----+\n",
      "|  0|hello|HELLO|\n",
      "|  1|world|WORLD|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, text: string]\n",
       "upper: String => String = $Lambda$4624/0x0000000841879840@7ff29c7\n",
       "upperUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4624/0x0000000841879840@7ff29c7,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(upper),true,true)\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val df = spark.createDataFrame(Seq((0, \"hello\"), (1, \"world\"))).toDF(\"id\", \"text\")\n",
    "val upper: String => String = _.toUpperCase\n",
    "val upperUDF = spark.udf.register(\"upper\", upper)\n",
    "df.withColumn(\"upper\", upperUDF(col(\"text\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Example - Individual Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+\n",
      "| id|       topic|                text|\n",
      "+---+------------+--------------------+\n",
      "|  0|    sci.math|        Hello, Math!|\n",
      "|  1|alt.religion|    Hello, Religion!|\n",
      "|  2| sci.physics|     Hello, Physics!|\n",
      "|  3|    sci.math|Hello, Math Revised!|\n",
      "|  4|    sci.math|         Better Math|\n",
      "|  5|alt.religion|                TGIF|\n",
      "+---+------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.sql.Row\n",
       "defined class Article\n",
       "articles: org.apache.spark.sql.DataFrame = [id: bigint, topic: string ... 1 more field]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "case class Article(id: Long, topic: String, text: String)\n",
    "\n",
    "val articles = spark.createDataFrame(Seq(\n",
    "    Article(0, \"sci.math\", \"Hello, Math!\"),\n",
    "    Article(1, \"alt.religion\", \"Hello, Religion!\"),\n",
    "    Article(2, \"sci.physics\", \"Hello, Physics!\"),\n",
    "    Article(3, \"sci.math\", \"Hello, Math Revised!\"),\n",
    "    Article(4, \"sci.math\", \"Better Math\"),\n",
    "    Article(5, \"alt.religion\", \"TGIF\"))).toDF\n",
    "\n",
    "articles.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+-----+\n",
      "| id|       topic|                text|label|\n",
      "+---+------------+--------------------+-----+\n",
      "|  0|    sci.math|        Hello, Math!|  1.0|\n",
      "|  1|alt.religion|    Hello, Religion!|  0.0|\n",
      "|  2| sci.physics|     Hello, Physics!|  1.0|\n",
      "|  3|    sci.math|Hello, Math Revised!|  1.0|\n",
      "|  4|    sci.math|         Better Math|  1.0|\n",
      "|  5|alt.religion|                TGIF|  0.0|\n",
      "+---+------------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic2Label: Boolean => Double = $Lambda$4826/0x0000000841922040@22d6b6c0\n",
       "toLabel: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4826/0x0000000841922040@22d6b6c0,DoubleType,List(Some(class[value[0]: boolean])),Some(class[value[0]: double]),Some(topic2Label),false,true)\n",
       "labelled: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, topic: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topic2Label: Boolean => Double = x => if (x) 1 else 0\n",
    "\n",
    "val toLabel = spark.udf.register(\"topic2Label\", topic2Label)\n",
    "\n",
    "val labelled = articles.withColumn(\"label\", toLabel($\"topic\".like(\"sci%\"))).cache\n",
    "\n",
    "labelled.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+-----+------------------------+\n",
      "|id |topic       |text                |label|words                   |\n",
      "+---+------------+--------------------+-----+------------------------+\n",
      "|0  |sci.math    |Hello, Math!        |1.0  |[hello,, math!]         |\n",
      "|1  |alt.religion|Hello, Religion!    |0.0  |[hello,, religion!]     |\n",
      "|2  |sci.physics |Hello, Physics!     |1.0  |[hello,, physics!]      |\n",
      "|3  |sci.math    |Hello, Math Revised!|1.0  |[hello,, math, revised!]|\n",
      "|4  |sci.math    |Better Math         |1.0  |[better, math]          |\n",
      "|5  |alt.religion|TGIF                |0.0  |[tgif]                  |\n",
      "+---+------------+--------------------+-----+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Tokenizer\n",
       "import org.apache.spark.ml.feature.RegexTokenizer\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_8f0b4eaac691, minTokenLength=1, gaps=true, pattern=\\s+, toLowercase=true\n",
       "tokenized: org.apache.spark.sql.DataFrame = [id: bigint, topic: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "val tokenized = tokenizer.transform(labelled)\n",
    "\n",
    "tokenized.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+-----+------------------------+------------------------------------+\n",
      "|id |topic       |text                |label|words                   |features                            |\n",
      "+---+------------+--------------------+-----+------------------------+------------------------------------+\n",
      "|0  |sci.math    |Hello, Math!        |1.0  |[hello,, math!]         |(5000,[4391,4731],[1.0,1.0])        |\n",
      "|1  |alt.religion|Hello, Religion!    |0.0  |[hello,, religion!]     |(5000,[2205,4731],[1.0,1.0])        |\n",
      "|2  |sci.physics |Hello, Physics!     |1.0  |[hello,, physics!]      |(5000,[4731,4945],[1.0,1.0])        |\n",
      "|3  |sci.math    |Hello, Math Revised!|1.0  |[hello,, math, revised!]|(5000,[869,2396,4731],[1.0,1.0,1.0])|\n",
      "|4  |sci.math    |Better Math         |1.0  |[better, math]          |(5000,[2396,2543],[1.0,1.0])        |\n",
      "|5  |alt.religion|TGIF                |0.0  |[tgif]                  |(5000,[987],[1.0])                  |\n",
      "+---+------------+--------------------+-----+------------------------+------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.HashingTF\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_9f0da6cbfb89, binary=false, numFeatures=5000\n",
       "hashed: org.apache.spark.sql.DataFrame = [id: bigint, topic: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.HashingTF\n",
    "\n",
    "val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "    .setNumFeatures(5000)\n",
    "\n",
    "val hashed = hashingTF.transform(tokenized)\n",
    "\n",
    "hashed.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+-----+--------------------+--------------------+\n",
      "| id|       topic|                text|label|               words|            features|\n",
      "+---+------------+--------------------+-----+--------------------+--------------------+\n",
      "|  1|alt.religion|    Hello, Religion!|  0.0| [hello,, religion!]|(5000,[2205,4731]...|\n",
      "|  2| sci.physics|     Hello, Physics!|  1.0|  [hello,, physics!]|(5000,[4731,4945]...|\n",
      "|  3|    sci.math|Hello, Math Revised!|  1.0|[hello,, math, re...|(5000,[869,2396,4...|\n",
      "|  4|    sci.math|         Better Math|  1.0|      [better, math]|(5000,[2396,2543]...|\n",
      "+---+------------+--------------------+-----+--------------------+--------------------+\n",
      "\n",
      "+---+------------+------------+-----+---------------+--------------------+\n",
      "| id|       topic|        text|label|          words|            features|\n",
      "+---+------------+------------+-----+---------------+--------------------+\n",
      "|  0|    sci.math|Hello, Math!|  1.0|[hello,, math!]|(5000,[4391,4731]...|\n",
      "|  5|alt.religion|        TGIF|  0.0|         [tgif]|  (5000,[987],[1.0])|\n",
      "+---+------------+------------+-----+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, topic: string ... 4 more fields]\n",
       "testDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, topic: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainDF, testDF) = hashed.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "trainDF.show\n",
    "\n",
    "testDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|       topic|label|prediction|\n",
      "+------------+-----+----------+\n",
      "|    sci.math|  1.0|       1.0|\n",
      "|alt.religion|  0.0|       1.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_3d0ed9511c1a\n",
       "model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_3d0ed9511c1a, numClasses=2, numFeatures=5000\n",
       "pred: org.apache.spark.sql.DataFrame = [topic: string, label: double ... 1 more field]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setMaxIter(20).setRegParam(0.01)\n",
    "\n",
    "val model = lr.fit(trainDF)\n",
    "\n",
    "val pred = model.transform(testDF).select(\"topic\", \"label\", \"prediction\")\n",
    "\n",
    "pred.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Example - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+-----+\n",
      "| id|       topic|                text|label|\n",
      "+---+------------+--------------------+-----+\n",
      "|  0|    sci.math|        Hello, Math!|  1.0|\n",
      "|  1|alt.religion|    Hello, Religion!|  0.0|\n",
      "|  2| sci.physics|     Hello, Physics!|  1.0|\n",
      "|  3|    sci.math|Hello, Math Revised!|  1.0|\n",
      "|  4|    sci.math|         Better Math|  1.0|\n",
      "+---+------------+--------------------+-----+\n",
      "\n",
      "+---+------------+----+-----+\n",
      "| id|       topic|text|label|\n",
      "+---+------------+----+-----+\n",
      "|  5|alt.religion|TGIF|  0.0|\n",
      "+---+------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainDF2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, topic: string ... 2 more fields]\n",
       "testDF2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, topic: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainDF2, testDF2) = labelled.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainDF2.show\n",
    "\n",
    "testDF2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|       topic|label|prediction|\n",
      "+------------+-----+----------+\n",
      "|alt.religion|  0.0|       1.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_35c833a4cf6f\n",
       "model2: org.apache.spark.ml.PipelineModel = pipeline_35c833a4cf6f\n",
       "pred: org.apache.spark.sql.DataFrame = [topic: string, label: double ... 1 more field]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "val model2 = pipeline.fit(trainDF2)\n",
    "\n",
    "val pred = model2.transform(testDF2).select(\"topic\", \"label\", \"prediction\")\n",
    "\n",
    "pred.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+\n",
      "|val1|val2|val3|     features|\n",
      "+----+----+----+-------------+\n",
      "|   1|   2|   3|[1.0,2.0,3.0]|\n",
      "|   4|   5|   6|[4.0,5.0,6.0]|\n",
      "|   7|   8|   9|[7.0,8.0,9.0]|\n",
      "+----+----+----+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "defined class Nums\n",
       "numsDF: org.apache.spark.sql.DataFrame = [val1: bigint, val2: bigint ... 1 more field]\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_240048322a6c, handleInvalid=error, numInputCols=3\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "va.transform(numsDF).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Continues Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|bucketizer_8b0d6a960ea0__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            3.0|\n",
      "|16.0|                            3.0|\n",
      "|17.0|                            3.0|\n",
      "|18.0|                            3.0|\n",
      "|19.0|                            3.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Bucketizer\n",
       "contDF: org.apache.spark.sql.DataFrame = [id: double]\n",
       "bucketBorders: Array[Double] = Array(-1.0, 5.0, 10.0, 15.0, 20.0)\n",
       "bucketer: org.apache.spark.ml.feature.Bucketizer = Bucketizer: uid=bucketizer_8b0d6a960ea0\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "\n",
    "val bucketBorders = Array(-1.0, 5.0, 10.0, 15.0, 20.0)\n",
    "val bucketer = new Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "defined class Nums\n",
       "numsDF: org.apache.spark.sql.DataFrame = [val1: bigint, val2: bigint ... 1 more field]\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_ba14a68f2691, handleInvalid=error, numInputCols=3\n",
       "nums: org.apache.spark.sql.DataFrame = [val1: bigint, val2: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "val nums = va.transform(numsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+-------------------------------------------+\n",
      "|val1|val2|val3|features     |scaled                                     |\n",
      "+----+----+----+-------------+-------------------------------------------+\n",
      "|1   |2   |3   |[1.0,2.0,3.0]|[0.3333333333333333,0.6666666666666666,1.0]|\n",
      "|4   |5   |6   |[4.0,5.0,6.0]|[1.3333333333333333,1.6666666666666665,2.0]|\n",
      "|7   |8   |9   |[7.0,8.0,9.0]|[2.333333333333333,2.6666666666666665,3.0] |\n",
      "+----+----+----+-------------+-------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_0ea01b8e3677\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "\n",
    "scaler.fit(nums).transform(nums).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "defined class Nums\n",
       "numsDF: org.apache.spark.sql.DataFrame = [val1: bigint, val2: bigint ... 1 more field]\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_7c4cbd56edad, handleInvalid=error, numInputCols=3\n",
       "nums: org.apache.spark.sql.DataFrame = [val1: bigint, val2: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "val nums = va.transform(numsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red|good|    45| 38.97187133755819|\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simpleDF: org.apache.spark.sql.DataFrame = [color: string, lab: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val simpleDF = spark.read.json(\"simple-ml.json\")\n",
    "\n",
    "simpleDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "lblIndxr: org.apache.spark.ml.feature.StringIndexer = strIdx_3f0b4194a072\n",
       "idxRes: org.apache.spark.sql.DataFrame = [color: string, lab: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val lblIndxr = new StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "\n",
    "val idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+--------+\n",
      "|color| lab|value1|            value2|labelInd|original|\n",
      "+-----+----+------+------------------+--------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|    good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|     bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|     bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|    good|\n",
      "|green|good|    12|14.386294994851129|     1.0|    good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|     bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|    good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|     bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|     bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|     bad|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|    good|\n",
      "|green|good|     1|14.386294994851129|     1.0|    good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|     bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|     bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|    good|\n",
      "|green|good|    12|14.386294994851129|     1.0|    good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|     bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|    good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|     bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|     bad|\n",
      "+-----+----+------+------------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.IndexToString\n",
       "labelReverse: org.apache.spark.ml.feature.IndexToString = idxToStr_3c67c6f00225\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.IndexToString\n",
    "\n",
    "val labelReverse = new IndexToString().setInputCol(\"labelInd\").setOutputCol(\"original\")\n",
    "\n",
    "labelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|colorInd|      one-hot|\n",
      "+--------+-------------+\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     2.0|    (2,[],[])|\n",
      "|     2.0|    (2,[],[])|\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     2.0|    (2,[],[])|\n",
      "|     2.0|    (2,[],[])|\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     1.0|(2,[1],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "|     0.0|(2,[0],[1.0])|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "lblIndxr: org.apache.spark.ml.feature.StringIndexer = strIdx_3903bce5d40f\n",
       "colorLab: org.apache.spark.sql.DataFrame = [color: string, colorInd: double]\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_8abc28fb989c\n",
       "ohe: org.apache.spark.ml.feature.OneHotEncoderModel = OneHotEncoderModel: uid=oneHotEncoder_8abc28fb989c, dropLast=true, handleInvalid=error\n",
       "encoded: org.apache.spark.sql.DataFrame = [colorInd: double, one-hot: vector]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val lblIndxr = new StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "\n",
    "val colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "\n",
    "val encoder = new OneHotEncoder().setInputCol(\"colorInd\").setOutputCol(\"one-hot\")\n",
    "\n",
    "val ohe = encoder.fit(colorLab)\n",
    "\n",
    "val encoded = ohe.transform(colorLab.select(\"colorInd\"))\n",
    "\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sales: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sales = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"sales.csv\").where(\"Description IS NOT NULL\")\n",
    "\n",
    "sales.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[white, hanging, heart, t-light, holder]  |\n",
      "|WHITE METAL LANTERN                |[white, metal, lantern]                   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[cream, cupid, hearts, coat, hanger]      |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[knitted, union, flag, hot, water, bottle]|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[red, woolly, hottie, white, heart.]      |\n",
      "|SET 7 BABUSHKA NESTING BOXES       |[set, 7, babushka, nesting, boxes]        |\n",
      "|GLASS STAR FROSTED T-LIGHT HOLDER  |[glass, star, frosted, t-light, holder]   |\n",
      "|HAND WARMER UNION JACK             |[hand, warmer, union, jack]               |\n",
      "|HAND WARMER RED POLKA DOT          |[hand, warmer, red, polka, dot]           |\n",
      "|ASSORTED COLOUR BIRD ORNAMENT      |[assorted, colour, bird, ornament]        |\n",
      "|POPPY'S PLAYHOUSE BEDROOM          |[poppy's, playhouse, bedroom]             |\n",
      "|POPPY'S PLAYHOUSE KITCHEN          |[poppy's, playhouse, kitchen]             |\n",
      "|FELTCRAFT PRINCESS CHARLOTTE DOLL  |[feltcraft, princess, charlotte, doll]    |\n",
      "|IVORY KNITTED MUG COSY             |[ivory, knitted, mug, cosy]               |\n",
      "|BOX OF 6 ASSORTED COLOUR TEASPOONS |[box, of, 6, assorted, colour, teaspoons] |\n",
      "|BOX OF VINTAGE JIGSAW BLOCKS       |[box, of, vintage, jigsaw, blocks]        |\n",
      "|BOX OF VINTAGE ALPHABET BLOCKS     |[box, of, vintage, alphabet, blocks]      |\n",
      "|HOME BUILDING BLOCK WORD           |[home, building, block, word]             |\n",
      "|LOVE BUILDING BLOCK WORD           |[love, building, block, word]             |\n",
      "|RECIPE BOX WITH METAL HEART        |[recipe, box, with, metal, heart]         |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Tokenizer\n",
       "tkn: org.apache.spark.ml.feature.Tokenizer = tok_b416b20ffddb\n",
       "tokenized: org.apache.spark.sql.DataFrame = [Description: string, DescOut: array<string>]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val tkn = new Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "\n",
    "val tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "\n",
    "tokenized.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |WithoutStops        |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, raw: array<string>]\n",
       "englishStopWords: Array[String] = Array(i, me, my, myself, we, our, ours, ourselves, you, your, yours, yourself, yourselves, he, him, his, himself, she, her, hers, herself, it, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, o...\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val df = spark.createDataFrame(Seq((0, Seq(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n",
    "                                   (1, Seq(\"Mary\", \"had\", \"a\", \"little\", \"lamb\")))).toDF(\"id\", \"raw\")\n",
    "\n",
    "val englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "val stops = new StopWordsRemover().setStopWords(englishStopWords).setInputCol(\"raw\").setOutputCol(\"WithoutStops\")\n",
    "\n",
    "stops.transform(df).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.CountVectorizer\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, words: array<string>]\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_42ce79b74a30\n",
       "fittedCV: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_42ce79b74a30, vocabularySize=3\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "\n",
    "val df = spark.createDataFrame(Seq((0, Array(\"a\", \"b\", \"c\")),\n",
    "                                   (1, Array(\"a\", \"b\", \"b\", \"c\", \"a\")))).toDF(\"id\", \"words\")\n",
    "\n",
    "val cvModel = new CountVectorizer().setInputCol(\"words\").setOutputCol(\"features\").setVocabSize(3).setMinDF(2)\n",
    "\n",
    "val fittedCV = cvModel.fit(df)\n",
    "\n",
    "fittedCV.transform(df).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.format(\"libsvm\").load(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label             |features                                                                                                                                                                                                                            |\n",
      "+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|-9.490009878824548|(10,[0,1,2,3,4,5,6,7,8,9],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715])|\n",
      "+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_68939f959cee\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_68939f959cee, numFeatures=10\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression().setMaxIter(10)\n",
    "val lrModel = lr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0073350710225801715,0.8313757584337543,-0.8095307954684084,2.441191686884721,0.5191713795290003,1.1534591903547016,-0.2989124112808717,-0.5128514186201779,-0.619712827067017,0.6956151804322931] Intercept: 0.14228558260358093\n",
      "RMSE: 10.16309157133015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@35e1924a\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "val trainingSummary = lrModel.summary\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class cancer\n",
       "trainData: org.apache.spark.sql.DataFrame = [x1: bigint, y: bigint]\n",
       "testData: org.apache.spark.sql.DataFrame = [x1: bigint, y: bigint]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class cancer(x1: Long, y: Long)\n",
    "val trainData = spark.createDataFrame(Seq(cancer(330, 1), cancer(120, 0), cancer(400, 1))).toDF\n",
    "val testData = spark.createDataFrame(Seq(cancer(500, 0))).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_4e824d6fcfe2, handleInvalid=error, numInputCols=1\n",
       "train: org.apache.spark.sql.DataFrame = [x1: bigint, y: bigint ... 1 more field]\n",
       "test: org.apache.spark.sql.DataFrame = [x1: bigint, y: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"x1\")).setOutputCol(\"features\")\n",
    "val train = va.transform(trainData)\n",
    "val test = va.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----------------------------------------+----------------------------------------+----------+\n",
      "|x1 |y  |features|rawPrediction                           |probability                             |prediction|\n",
      "+---+---+--------+----------------------------------------+----------------------------------------+----------+\n",
      "|500|0  |[500.0] |[-1.6828468484311974,1.6828468484311974]|[0.15671886712181188,0.8432811328781882]|1.0       |\n",
      "+---+---+--------+----------------------------------------+----------------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_f382c39e9fc8\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_f382c39e9fc8, numClasses=2, numFeatures=1\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setFeaturesCol(\"features\").setLabelCol(\"y\")\n",
    ".setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(train)\n",
    "lrModel.transform(test).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "3 x 4 CSCMatrix\n",
      "(1,2) -0.7666333131801362\n",
      "(0,3) 0.3049998979124714\n",
      "(1,3) -0.38544484160713977\n",
      "Intercepts: \n",
      "[0.051925800207288285,-0.12619173083598836,0.07426593062870007]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_bbb115a5c115\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_bbb115a5c115, numClasses=3, numFeatures=4\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = spark.read.format(\"libsvm\").load(\"multiclass_data.txt\")\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(training)\n",
    "\n",
    "println(s\"Coefficients: \\n${lrModel.coefficientMatrix}\")\n",
    "println(s\"Intercepts: \\n${lrModel.interceptVector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
